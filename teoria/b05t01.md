# Pruebas (_testing_)

En el ciclo de desarrollo, es natural correr algún programa que compruebe que
nuestros algoritmos hacen lo que deben hacer. Esto son las pruebas, o _tests_.

Probar un software implica, además de comprobar que funciona correctamente,
poder automatizar este proceso.

En los últimos años, las pruebas automatizadas han adquirido una importancia
notable, no sólo como herramienta de verificación, sino también como
**herramienta de diseño** y es común encontrar metodologías que ponen especial
énfases en las pruebas como el **diseño dirigido por tests**
(o [_TDD_](https://en.wikipedia.org/wiki/Test-driven_development), por sus
siglas en inglés) o el **diseño dirigido por comportamiento**
(o [_BDD_](https://en.wikipedia.org/wiki/Behavior-driven_development)).

Ambas técnicas refuerzan la idea del diseño _top-down_, donde **primero
definimos qué queremos hacer y luego implementamos la funcionalidad**. _BDD_ va
un paso más allá, introduciendo una metodología para escribir pruebas de forma
que personas no técnicas puedan definirlas.

* [Rediscovering TDD](https://www.quora.com/Why-does-Kent-Beck-refer-to-the-rediscovery-of-test-driven-development-Whats-the-history-of-test-driven-development-before-Kent-Becks-rediscovery)*
* [Introducing BDD](https://dannorth.net/introducing-bdd/)*
* [Specialized tooling support](https://en.wikipedia.org/wiki/Behavior-driven_development#Specialized_tooling_support) for BDD

## Anatomía de una prueba

Por lo general, una prueba consta de las siguientes partes:

1. **Nombre**. Describe qué prueba el _test_.
2. **Expectativa**. Describe qué resultado espera el _test_.
3. **Preparación del entorno**. Configura el entorno con las condiciones
iniciales.
4. **Ejecución de la prueba**. Ejercita el código que queremos probar.
5. **Comprobación del resultado**. Compara el resultado contra algún valor de
referencia esperado.
6. **Limpieza del entorno**. Restaura el entorno a las condiciones en las que
se encontraba antes de la preparación.

Las herramientas de pruebas de los distintos lenguajes intentan que la
escritura de las partes 1, 2, 4 y 5 sea ágil y tratan de extraer en común la
configuración de los test.

## El módulo `unittest`

Python ofrece el módulo `unittest` para la creación de pruebas.

1. Crea un proyecto nuevo y añade el módulo `test_fibonacci.py`. Dentro,
escribe:

    ```python
    import unittest
    from fibonacci import fibonacci

    class TestFibonacciSequence(unittest.TestCase):

        def test_base_case_0(self):
            """Should return 1 for index 0."""
            expected_result = 1
            actual_result = fibonacci(0)
            self.assertEqual(expected_result, actual_result)


    if __name__ == '__main__':
        unittest.main()
    ```

2. Además, a la misma altura crea un fichero `fibonacci.py` y escribe:

    ```python
    def fibonacci(index):
        if index in (0, 1):
            return 1

        return fibonacci(index - 1) + fibonacci(index - 2)
    ```

3. Ahora ejecuta el siguiente comando en la raíz del proyecto:

    ```bash
    $ python3 -m unittest
    ```

    Esto descubrirá automáticamente los test que tengas buscando clases que
    hereden de `unittest.TestCase` en aquellos ficheros que comiencen por
    `test*`.

    El resultado debería ser similar a:

    ```bash
    $ python -m unittest
    .
    ----------------------------------------------------------------------
    Ran 1 test in 0.000s

    OK
    ```

4. Si queremos que los _tests_ arrojen algo más de información, podemos usar:

    ```bash
    $ python -m unittest -v
    ```

    La opción `-v` configura el lanzador de pruebas o _test runner_ en modo
    detallado y la salida luce ahora:

    ```bash
    $ python -m unittest -v
    test_base_case_0 (test_fibonacci.TestFibonacciSequence)
    Should return 1 for index 0. ... ok

    ----------------------------------------------------------------------
    Ran 1 test in 0.000s

    OK
    ```

5. Por lo general, las pruebas se situan en una carpeta `tests` que replican
el árbol de directorios y módulos del paquete principal del proyecto. Así que
si tu proyecto tiene esta pinta...

    ```
    .
    └── awesomelib
        ├── __init__.py
        ├── fibonacci.py
        └── utils
            ├── __init__.py
            └── network.py
    ```

    Junto con el directorio de tests debería ser algo como:

    ```
    .
    ├── awesomelib
    │   ├── __init__.py
    │   ├── fibonacci.py
    │   └── utils
    │       ├── __init__.py
    │       └── network.py
    └── tests
        ├── __init__.py
        ├── test_fibonacci.py
        └── utils
            ├── __init__.py
            └── test_network.py
    ```

    Es importante que los directorios en `tests` sean paquetes de Python, así
    que deben incluir los ficheros `__init__.py` convenientes.

6. Añade un segundo y tercer test para cubrir el otro caso base y el caso
recursivo (por ejemplo para el índice `20` el resultado es `10946`). Por
ejemplo, el segundo caso base queda:

    ```python
    def test_base_case_1(self):
        """Should return 1 for index 1."""
        expected_result = 1
        actual_result = fibonacci(1)
        self.assertEqual(expected_result, actual_result)
    ```

7. A veces algunos tests son muy parecidos, como les ocurre a los dos del caso
base. Podemos agruparlos en subpruebas con el método `subTest`:

    ```python
    def test_base_cases(self):
        """Should return 1 for indices 0 and 1."""
        expected_result = 1
        for index in (0, 1):
            with self.subTest(index=index):
                actual_result = fibonacci(index)
                self.assertEqual(expected_result, actual_result)
    ```

    Corre los tests para ver cómo el informador (_test reporter_) presenta
    el hecho de estar ejecutando subpruebas.

    Ahora modifica el código fuente para que falle intencionadamente cuando
    reciba el índice `0`. ¿Cómo se informa? Restaura el código cuando termines.

### Comprobaciones

Fíjate en el método `self.assertEqual`. Python permite muchos tipos de
comprobaciones que puedes encontrar en la documentación. Cuando quieras hacer
una comprobación, trata de encontrar el método más semántico posible.

1. Para que compruebes la importancia de utilizar siempre el método más
semántico, considera la siguiente prueba:

    ```python
    def test_assert_in(self):
        self.assertIn(1, ['a', 'b', 'c'])
    ```

    Esta prueba falla indicando:

    ```
    AssertionError: 1 not found in ['a', 'b', 'c']
    ```

    Sin embargo, modifica ahora el test para usar una comprobación más general:

    ```python
    def test_assert_in(self):
        self.assertTrue(1 in ['a', 'b', 'c'])
    ```

    ¿Qué observas? ¿Qué mensaje es más informativo?

2. Existe otro tipo de comprobaciones, las comprobaciones contextuales. Por
ejemplo, añade un test para comprobar el funcionamiento de la función cuando
recive un índice negativo:

    ```python
    def test_negative_index(self):
        """Should raise a ValueError exception."""
        with self.assertRaises(ValueError):
            _ = fibonacci(-1)
    ```

    El estado actual de la función `fibonacci` no hace que falle el test
    sino que sea erróneo, indicando que se alcanza el límite de la recursión.
    Discutiremos este resultado más adelante. Por el momento, modifica la
    función para que devuelva `None` cuando reciba un índice negativo y observa
    cómo falla el _test_.

    ```
    AssertionError: ValueError not raised
    ```

    Cambia ahora la función para que lance la excepción `ValueError` que
    esperábamos.

3. Modifica la función `fibonacci` para que el mensaje de la excepción
`ValueError` sea `'The index is negative.'`. Podemos recuperar la excepción
y comprobar el mensaje mediante:

    ```python
    def test_negative_index(self):
        """Should raise a ValueError exception."""
        with self.assertRaises(ValueError) as cm:
            _ = fibonacci(-1)

        self.assertIn('is negative', str(cm.exception))
    ```

* [Lista de comprobaciones](https://docs.python.org/3/library/unittest.html#assert-methods)*

### Resultados de un _test_

Hemos visto 3 tipos de resultados hasta el momento: éxito, fracaso y error.

El éxito ocurre cuando el test termina. El fracaso ocurre si algún método
`assert*` falla. De hecho, lo que ocurre es que el método lanza una excepción
del tipo `AssertionError`. El error ocurre cuando se lanza una excepción que
**no es `AssertionError`**.

1. Podemos omitir (_skip_) un test usando el decorador `@unittest.skip()`,
junto con una razón:

    ```python
    @unittest.skip('Teaching purposes')
    def test_skip_decorator(self):
        """Should not run."""
        self.assertTrue(False)  # would fail if run
    ```

    Si ejecutas las pruebas, el resultado será:

    ```bash
    $ python -m unittest
    ...s
    ----------------------------------------------------------------------
    Ran 4 tests in 0.000s

    OK (skipped=1)
    ```

    Fíjate que el informe indica una prueba omitida. El modo detallado
    indica las razones por las que ignorar el test. Compruébalo.

2. Lo normal no es usar `@unittest.skip()` sino `@unittest.skipIf()` que
permite evaluar una condición y omitir el test en caso de que la condición
sea verdadera. Por ejemplo:

    ```python
    from sys import version_info
    @unittest.skipIf(version_info.minor < 6, 'Python version too old.')
    def test_skipIf_decorator(self):
        """Should run in Python 3.6 and above only."""
        ...
    ```

3. Es también posible que durante un tiempo, se espere que cierto código falle.
Para indicar esta situación se utiliza el decorador
`@unittest.expectedFailure`:

    ```python
    @unittest.expectedFailure
    def test_intended_failure(self):
        self.assertTrue(False)
    ```

    El informe señala en este caso que el test falló... como se esperaba:

    ```bash
    $ python -m unittest
    .x..
    ----------------------------------------------------------------------
    Ran 4 tests in 0.001s

    OK (expected failures=1)
    ```

La siguiente tabla resume los posibles resultados de las pruebas:

| Resultado | Cuándo se produce                          | Símbolo |
|-----------|--------------------------------------------|---------|
|Éxito      | El test termina.                           | `.`     |
|Fallo      | Falla un método `assert*`.                 | `F`     |
|Error      | Se produce una excepción.                  | `E`     |
|Omisión    | Se usan los decoradores `skip*`.           | `s`     |
|Fallo esperado | Se usa el decorador `expectedFailure`. | `x`     |

### Configuración

Los _tests_ pueden configurarse, por ejemplo, si es necesario falsear algunos
objetos con los que las funcionalidad que queremos probar tiene que
relacionarse. O si tenemos que preparar una base de datos de una determinada
forma antes de probar el código que manipula dicha base de datos.

Para ello podemos usar los métodos `setUp()` y `tearDown()` que se ejecutan
**justo antes y después de cada test, respectivamente**. Por ejemplo, considera
el siguiente listado con funcionalidad y _test_, todo incluído:

```python
import unittest

class DataBaseController:

    def __init__(self, db=None):
        self._db = db if db is not None else set()

    def add(self, name):
        self._db.add(name)

    def remove(self, name):
        self._db.remove(name)


class TestDataBaseController(unittest.TestCase):

    def setUp(self):
        self._db = set(['Salva'])
        self._controller = DataBaseController(self._db)

    def test_add_method(self):
        """Should add a new item to the database."""
        self._controller.add('Ziltoid')
        self.assertIn('Ziltoid', self._db)
        self.assertEqual(2, len(self._db))

    def test_remove_method(self):
        """Should remove an existent item from the database."""
        self._controller.remove('Salva')
        self.assertNotIn('Salva', self._db)
        self.assertEqual(0, len(self._db))
```

### Nuevas comprobaciones

Es conveniente que la fase de comprobación de un _test_, lo sea de una sola
cosa. Si tu listado de comprobaciones comienza a crecer o incluye lógica,
considera crear una nueva comprobación. Por ejemplo:

```python
import unittest

class TestFileUtils(unittest.TestCase):

    def assertAllFilesVisible(self, filenames):
        """Raises ``AssertionError`` if some filename starts with `.`."""
        for name in filenames:
            if name.startswith('.'):
                raise AssertionError(f'Filename "{name}"" is a hidden file.')

    def test_all_files_visible(self):
        """Should fail pointing to ``.DS_Store``."""
        self.assertAllFilesVisible(['a.jpg', 'b.png', '.DS_Store'])
```

## El módulo `doctest`

## Tipos de pruebas

No todas las pruebas verifican las mismas cosas.
