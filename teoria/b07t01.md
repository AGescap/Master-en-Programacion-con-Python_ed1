# Python de alto rendimiento

Python se pensó para ocultar los detalles de implementación del lenguaje y que
el desarrollador pudiera centrarse en los aspectos prácticos de la resolución
de problemas.

Desafortunadamente, la abstracción está reñida con el rendimiento y los
mecanismos que empleamos para mantener un diseño modular, reutilizable
y cohesionado, pero sin acoplamiento o dependencias fuertes, añaden una
sobrecarga que puede ser evidente en ciertas situaciones. Al fin y al cabo los
procesadores se diseñaron para optimizar la ejecución de código ensamblador,
no Python.

Los optimizadores de código de los lenguajes compilados hacen precisamente
esto: eliminan la abstracción y acoplan los subsistemas del lenguaje tomando
decisiones que suelen intercambiar especio por velocidad.

Afortunadamente para nosotros, la compilación no es el único recurso. La
naturaleza misma del problema puede arrojar pistas de mejores estrategias
para resolver el mismo, sin tener que descender al mundo de los lenguajes
compilados.

## Caracterización de aplicaciones (_profiling_)

Para poder optimizar un problema, primero debemos caracterizarlo. Existen
algunas herramientas en el ecosistema Python para realizar este análisis y
el propio Python incluye el módulo `cProfiler` para este fin.

1. Crea un fichero `profiling.py` y añade el siguiente listado:

    ```python
    import time

    def improvable():
        for i in range(10000000):
            _ = i**2
        time.sleep(2)

    def not_improvable():
        time.sleep(1)

    def main():
        not_improvable()
        improvable()

    if __name__ == '__main__':
        main()
    ```

2. Ahora, desde una línea de comandos, ejecuta:

    ```bash
    $ python -m cProfile profiling.py
    ```

    El resultado tendrá una pinta similar a:

    ```bash
    $ python -m cProfile profiling.py
             8 function calls in 6.117 seconds

       Ordered by: standard name

       ncalls  tottime  percall  cumtime  percall filename:lineno(function)
            1    0.000    0.000    6.117    6.117 profiling.py:1(<module>)
            1    0.000    0.000    6.117    6.117 profiling.py:11(main)
            1    3.108    3.108    5.112    5.112 profiling.py:3(improvable)
            1    0.000    0.000    1.004    1.004 profiling.py:8(not_improvable)
            1    0.000    0.000    6.117    6.117 {built-in method builtins.exec}
            2    3.008    1.504    3.008    1.504 {built-in method time.sleep}
            1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
    ```

    Donde existe una fila por cada función involucrada en la ejecución y
    los campos significan:

    - `ncalls` es el número de llamadas a la función.
    - `tottime` es el tiempo total pasado en esa función, sin contar el tiempo
    pasado en las sub-llamadas.
    - `percall` es la media de tiempo total por llamada.
    - `cumtime` es el tiempo total empleado en esa función, incluyendo
    sub-llamadas.
    - `percall` es la media del tiempo acumulado por llamada.
    - `filename:lineno(function)` indica dónde se produce la inversión de
    tiempo.

3. El módulo `cPython` permite grabar exportar esta información:

    ```bash
    $ python -m cProfile -o data.prof profiling.py
    ```

    Y utilizar
    [SnakeViz](https://jiffyclub.github.io/snakeviz/) o
    [tuna](https://github.com/nschloe/tuna)
    para visualizar el resultado:

    ```bash
    $ pip install tuna snakeviz
    $ snakeviz data.prof
    ```

    Cuidado, porque en las interfaces gráficas puede darse una interpretación
    distinta a los mismos nombre. La mayoría llama "tiempo total" al tiempo
    invertido por una función y las sub-llamadas. Cuando esto pasa, el tiempo
    empleado sólo en la función se suele llamar "tiempo local" o se marca de
    alguna forma.

Los gráficos de llamas son útiles porque permiten visualizar fracciones de
tiempo. En lenguajes interpretados, la mayor parte del fondo de las gráficas
son llamadas a las funciones de la biblioteca estándar, muchas de ellas
nativas y, por tanto, "inmejorables".

Conviene primero buscar las funciones que consumen más tiempo acumulado y,
de entre ellas, comenzar por las que emplean más tiempo en sí mismas.

## La ley de Amdahl

La ley de Amdahl dice:

> La mejora en la latencia de un sistema está limitada por la porción de
> tiempo en que no obtenemos ninguna ganancia.

La ley de Amdahl se expresa matemáticamente como `1/(1 - p)` donde `p` es la
**fracción de tiempo en la que podemos aplicar la mejora**.

Por ejemplo, supón que caracterizamos un software y descubrimos que el 80%
del tiempo lo empleamos en una función fácilmente paralelizable. Cuestión de
arrojar más núcleos al problema. Lo que dice la ley de Amdahl es que la mejora
en velocidad está limitada por ese 20% en el que no podemos aplicar la mejora.
En particular, la mejora no puede ser superior a `1/(1 - 0.8) = 5`. O lo que
es lo mismo, **el sistema nunca será más de 5 veces más rápido**.

La mejora real del sistema se calcula en base a la mejora local, según la
expresión `S = 1/((1 - p) + p/s)` donde `S` es la mejora global (_global
speedup_) y `p` vuelve a ser la fracción del tiempo donde podemos aplicar
la mejora local `s` (_local speedup_).

## Paralelismo y multiprocesamiento

¿Permite Python ejecución paralela?

La respuesta rápida es no. Sin embargo podemos ser algo más transigentes y
plantearnos otra pregunta, más general:

¿Permite Python ejecución concurrente?

En este caso **la respuesta es sí**. La diferencia entre paralelismo y
concurrencia es sutil aunque relevante. La **concurrencia se produce cuando
más de una tarea progresa al mismo tiempo**, sin esperar las unas a la
finalización de las otras. El **paralelismo se produce cuando dos tareas se
ejecutan al mismo tiempo**.

Python **soporta múltiples tareas (threads) pero sólo una de ellas puede estar
ejecutándose a la vez en el intérprete**. Quien posee la ejecución en cada
momento, se dice que está en posesión del
[GIL](https://wiki.python.org/moin/GlobalInterpreterLock).
Sin embargo, Python interrumpirá las tareas automáticamente y bajo ciertas
condiciones para dejar que otras tareas progresen antes de que la actual
termine.

Ojo, es el intérprete de Python el que no puede ejecutar más de una tarea al
mismo tiempo. Si la ejecución ha salido fuera del intérprete (a una llamada
del sistema nativo en C, por ejemplo), esta restricción no aplica y la
ejecución depende del sistema.

* [GIL: Todo lo que quisiste saber y no te atreviste a preguntar](https://www.youtube.com/watch?v=50eOwz9lek4&list=PLKxa4AIfm4pUQX9ePOy3KEpENDC331Izi&index=36)
* La [asincronía ilustrada en un caso de WebVR](https://hacks.mozilla.org/2017/11/a-super-stable-webvr-user-experience-thanks-to-firefox-quantum/).

### Concurrencia multi-hilo

1. Reemplaza el código de `profiling.py` (o crea un fichero nuevo) con
este otro:

    ```python
    from urllib import request

    def get(url):
        response = request.urlopen(url)
        print(f'Body at {url}:\n {response.read()}')

    def main():
        urls = [
            'https://raw.githubusercontent.com/python/cpython/master/README.rst',
            'https://raw.githubusercontent.com/rust-lang/rust/master/README.md',
            'https://raw.githubusercontent.com/ruby/ruby/master/README.md'] * 80

        list(map(get, urls))

    if __name__ == '__main__':
        main()
    ```

    Caracterízalo y visualiza los resultados. ¿Qué observas?

2. Ahora cambia el código para que utilice hilos:

    ```python
    from urllib import request
    from concurrent.futures import ThreadPoolExecutor

    def get(url):
        response = request.urlopen(url)
        print(f'Body at {url}:\n {response.read()}')

    def main():
        urls = [
            'https://raw.githubusercontent.com/python/cpython/master/README.rst',
            'https://raw.githubusercontent.com/rust-lang/rust/master/README.md',
            'https://raw.githubusercontent.com/ruby/ruby/master/README.md'] * 80

        with ThreadPoolExecutor(max_workers=5) as executor:
            executor.map(get, urls)

    if __name__ == '__main__':
        main()
    ```

    Caracterízalo **en un fichero de salida distinto** y visualiza los
    resultados. Compáralos con los de antes.

### Paralelismo multiproceso

1. Considera el siguiente código:

### ¿Cuándo usar paralelismo y cuándo usar concurrencia?

En general, tendrás que distinguir si tus programas están constreñidos por la
CPU o por la entrada y salida. Estas situaciones se conocen respectivamente
como [_CPU Bound_](https://en.wikipedia.org/wiki/CPU-bound) e
[_I/O Bound_](https://en.wikipedia.org/wiki/I/O_bound).

Si tu aplicación es _I/O Bound_ pasará más tiempo fuera del intérprete de
Python que dentro. En este caso, cualquiera de las aproximaciones es válida
aunque la concurrencia es más ligera y no implica al sistema operativo.

Si tu aplicación es _CPU Bound_, entonces pasará más tiempo en el intérprete
de Python que fuera. Es lo que pasa con los algoritmos matemáticos de solución
de problemas algebráicos o de cálculo. En este caso tienes que llevarte la
gestión de la multitarea al sistema operativo con multiprocesamiento porque
el intérprete sólo permitirá a un hilo operar al mismo tiempo.

## Intérpretes más rápidos: PyPy

## Compiladores "al vuelo" (_JIT compilers_): Numba

## Extensiones nativas